import asyncio
import os
import sys
from datetime import datetime

# ç¡®ä¿å¯ä»¥å¯¼å…¥ backend ä¸‹çš„æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

import acquisition.info_acquisition as info_acquisition
import acquisition.wechat_acquisition as wechat_acquisition
import analysis.info_analysis as info_analysis
import reporting.report_generation as report_generation
import reporting.wecom_push as wecom_push
import config

async def run_smoke_test():
    print("ğŸš€ å¯åŠ¨è½»é‡çº§åŒæ­¥æµ‹è¯• (Smoke Test)...")
    
    # 1. å®šä¹‰ç²¾ç®€æº (2ä¸ªå›½å†…ç½‘ç«™, 3ä¸ªå›½å¤–ç½‘ç«™)
    test_sources = [
        # å›½å†… (CCCC)
        {
            "name": "ä¸­äº¤ä¸Šæµ·èˆªé“å±€",
            "url": "https://www.cccc-sdc.com/cccc-sdc/channels/2207.html",
            "type": "web",
            "selector": ".news-list, .content-list, body",
            "max_links": 2
        },
        {
            "name": "ä¸­äº¤å¤©èˆªå±€",
            "url": "https://www.zjthbh.com/tjsj/channels/870.html",
            "type": "web",
            "selector": ".news-list, .content-list, body",
            "max_links": 2
        },
        # å›½å¤– (RSS + Web)
        {
            "name": "Dredging Today",
            "url": "https://dredgingtoday.com/feed/",
            "type": "rss"
        },
        {
            "name": "DredgeWire",
            "url": "https://dredgewire.com/feed/",
            "type": "rss"
        },
        {
            "name": "IADC Dredging",
            "url": "https://www.iadc-dredging.com/news/",
            "type": "web",
            "selector": "article",
            "max_links": 2
        }
    ]

    # [é˜¶æ®µ1] é‡‡é›†
    print(">>> [é˜¶æ®µ1] æ­£åœ¨é‡‡é›†ç²¾ç®€æºæ•°æ®...")
    raw_items = []
    
    # é‡‡é›† Web/RSS
    from playwright.async_api import async_playwright
    async with async_playwright() as p:
        browser = await info_acquisition.launch_chromium(p)
        context = await browser.new_context(
            viewport={"width": 1280, "height": 800},
            ignore_https_errors=True # å¿½ç•¥ HTTPS è¯ä¹¦é”™è¯¯ (å¦‚å¤©èˆªå±€å®˜ç½‘)
        )
        
        for s in test_sources:
            if s['type'] == 'rss':
                items = await info_acquisition.fetch_rss(s['url'], hours=24, source_name=s['name'])
                raw_items.extend(items[:2]) # æ¯ä¸ªæºåªå–2æ¡
            elif s['type'] == 'web':
                items = await info_acquisition.fetch_web_index(context, s)
                raw_items.extend(items[:2])
        
        await browser.close()

    # å¾®ä¿¡å…¬ä¼—å· (è‡ªåŠ¨ä» session æ–‡ä»¶åŠ è½½)
    wechat_biz_list = [
        {"name": "ä¸­äº¤ç–æµš", "fakeid": "MzI1NzYwNTQ5Ng=="},
        {"name": "ä¸­äº¤å¤©èˆªå±€", "fakeid": "MzA5NTU2NTYyNQ=="}
    ]
    wechat_items = wechat_acquisition.wechat_scraper.batch_get_articles(wechat_biz_list, count_per_biz=1)
    if wechat_items:
        print(f"æˆåŠŸè·å– {len(wechat_items)} æ¡å¾®ä¿¡å…¬ä¼—å·æ–°é—»")
        raw_items.extend(wechat_items)

    print(f"å…±é‡‡é›†åˆ° {len(raw_items)} æ¡æ½œåœ¨æ–°é—»")

    if not raw_items:
        print("æœªé‡‡é›†åˆ°ä»»ä½•æ–°é—»ï¼Œæµ‹è¯•ç»“æŸã€‚")
        return

    # 3. åˆ†ææ•°æ®
    print(">>> [é˜¶æ®µ2] æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ (ä»…åˆ†æå‰ 3 æ¡ä»¥èŠ‚çœ Token)...")
    # ä¸ºäº†æµ‹è¯•å¿«é€Ÿï¼Œæˆ‘ä»¬åªåˆ†æå‰ 3 æ¡
    items_to_process = raw_items[:3]
    results = await info_analysis.process_items(items_to_process)

    # 4. ä¿å­˜å¹¶ç”ŸæˆæŠ¥å‘Š
    print(">>> [é˜¶æ®µ3] æ­£åœ¨ä¿å­˜æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š...")
    report_generation.save_history(results)
    report_generation.generate_report(results)

    # 5. æ¨é€è‡³ä¼ä¸šå¾®ä¿¡
    print(">>> [é˜¶æ®µ4] æ­£åœ¨æ¨é€è‡³ä¼ä¸šå¾®ä¿¡...")
    wecom_push.push_daily_report()
    
    print("\nâœ… è½»é‡çº§åŒæ­¥æµ‹è¯•å®Œæˆï¼")
    print(f"å®¡è®¡æ–‡æ¡£: {config.REPORT_FILE}")

if __name__ == "__main__":
    asyncio.run(run_smoke_test())
